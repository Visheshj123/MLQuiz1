{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1A: \n",
    "\n",
    "According to the article the sensitivity was 90% and the specificity was 95%. This means the testing kits were \n",
    "able do a good job at correctly identifying those with and without antibodies, however having a positive \n",
    "predictive value of 49% means that there were too many false positives. In other words, the testing kit \n",
    "over-diagnosed people with having the antibodies. The reason you get high recall values and low precision values\n",
    "is because the sample sizes for those who were actually negative and actually positive varied drastically. \n",
    "See example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](Picture1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "print(randrange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [randrange(2) for x in range(1,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [randrange(2) for x in range(1,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.48148148, 0.33333333]),\n",
       " array([0.46428571, 0.34883721]),\n",
       " array([0.47272727, 0.34090909]),\n",
       " array([56, 43]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Interpretation: \n",
    "\n",
    "In this problem I am addressing a binary classification problem. To simulate true values and model predictions I\n",
    "randomly generated two arrays containing 1 and 0. To evaluate the model I used recall, precision, and f-score. \n",
    "Recall will communicate the accuracy for each class and precision will inform me if my model over/under assigned \n",
    "for a particular label. There is normally a trade off between these two metrics so f-score will give me a single value\n",
    "that helps me determine how \"balanced\" the recall and precision are for both classifications. Finally I use \n",
    "support to give me a sense how balanced my dataset is and to what degree it is influencing my other metrics.\n",
    "\n",
    "\n",
    "For label '0' we have a precision of 0.48 and recall of 0.46. This tells me that when my model predicts label '0' \n",
    "it's correct about half the time and the amount of incorrect overassignment for this label is roughly equal\n",
    "to the number of correct assignments. For label '1' Not only is my model poor at accurately predicting this label\n",
    "(34.8%), it also tends to predict a label of '0' a majority of the time. From these observtions I can conclude\n",
    "that my model is heavily bias toward the '0' label. The fscore for each label (0.47 and 0.34) give me an average \n",
    "single metric to evaluate my model's performance for both labels. In this case the fscore confirms that my model is\n",
    "better at evaulating '0' than '1'. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
